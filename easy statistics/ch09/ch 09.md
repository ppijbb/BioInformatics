
## 9장 회귀분석

### 9.1 최소제곱법

```
x = c(1, 2, 3, 4)
y = c(2, 3, 5, 4)

f = function(arg) {
  a = arg[1]
  b = arg[2]
  t = a * x + b
  sum((y - t)^2)
}

optim(c(1, 1), f)  # 초깃값(a,b)=(1,1)부터 시작해 f를 최소화한다
```

```
> optim(c(1, 1), f)$par
[1] 0.7999722 1.5000885
```

```
> r = lm(y ~ x)
> summary(r)

Call:
lm(formula = y ~ x)

Residuals:
   1    2    3    4
-0.3 -0.1  1.1 -0.7

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   1.5000     1.1619   1.291    0.326
x             0.8000     0.4243   1.886    0.200

Residual standard error: 0.9487 on 2 degrees of freedom
Multiple R-squared:   0.64,     Adjusted R-squared:   0.46
F-statistic: 3.556 on 1 and 2 DF,  p-value: 0.2
```

### 9.2 회귀의 유래

```
x = rnorm(40);  y = rnorm(40)
plot(x, y, pch=16, main=cor(x, y))
abline(lm(y ~ x))
```

```
par(mgp=c(2, 0.8, 0)) # title and axis margins. default: c(3,1,0)
par(mar=c(2, 2, 1, 1) + 0.1) # bottom, left, top, right margins. default: c(5,4,4,2)+0.1
set.seed(1)
a = rnorm(100);  b = rnorm(100);  c = rnorm(100)
x = (a + c) / sqrt(2)
y = (b + c) / sqrt(2)
plot(x, y, pch=16, xlim=c(-3, 3), ylim=c(-3, 3), asp=1)
abline(lm(y ~ x))
abline(0, 1, lty=2)
```

### 9.3 예: 다섯 번째의 힘

```
> x = c(0.94, 0.50, 0.00, -0.74, -0.86, -1.42, -1.71)
> y = c(0.4, 0.4, 0.0, -0.3, -0.5, -0.7, -1.0)
> e = c(0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2)
> plot(x, y, type="p", pch=16, ylim=range(c(y-e, y+e)),
+ xlab=expression(10^3 * Delta * (B/mu)),
+ ylab=expression(10^8 * Delta * kappa))
> arrows(x, y-e, x, y+e, length=0.03, angle=90, code=3)
```

```
> r = lm(y ~ x, weights=1/e^2)  # 오차가 일정하다면 r = lm(y ~ x)
```

```
> summary(r)

Call:
lm(formula = y ~ x, weights = 1/e^2)

Weighted Residuals:
      1       2       3       4       5       6       7
-0.9404  0.6282 -0.2603  0.3495 -0.3084  0.2882 -0.3850

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.05207    0.03867   1.347    0.236
x            0.57022    0.04294  13.281 4.33e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5992 on 5 degrees of freedom
Multiple R-squared:  0.9724,    Adjusted R-squared:  0.9669
F-statistic: 176.4 on 1 and 5 DF,  p-value: 4.327e-05
```

```
> abline(r)
```

```
> sum((r$residuals / e)^2)
```

```
> pchisq(1.795245, df=5)
```

```
r = lm(y ~ x - 1, weights=1/e^2)
```

### 9.4 푸아송 회귀

```
f = function(arg) {
    a = arg[1]
    b = arg[2]
    lambda = a * x + b
    -sum(y * log(lambda) - lambda)
}
optim(c(1, 1), f)
```

```
glm(y ~ x, family=poisson(link="identity"))
```

### 9.5 푸아송 회귀와 비슷한 방법과 등가 방법

```
f = function(arg) {
    a = arg[1]
    b = arg[2]
    lambda = a * x + b
    sum((y - lambda)^2 / y)
}
optim(c(1, 1), f)
```

```
lm(y ~ x, weights=1/y)
```

```
f = function(arg) {
    a = arg[1]
    b = arg[2]
    lambda = a * x + b
    sum((y - lambda)^2 / lambda)
}
optim(c(1, 1), f)
```

```
w = c(1, 1, 1, 1)    # 적당한 초깃값
for (i in 1:10) {    # 수렴할 때까지 계속한다
    r = lm(y ~ x, weights=w)
    lambda = predict(r)
    print(c(as.numeric(r$coef), -sum(y*log(lambda)-lambda)))
    w = 1 / lambda
}
```

```
weights=varPower(fixed=0.5)
```

```
library(nlme)
data = data.frame(y=y, x=x)
gnls(y ~ a * x + b, data=data,
     start=list(a=0.9,b=1.3), weights=varPower(fixed=0.5))
```

### 9.6 푸아송 회귀의 적합도

```
> x = c(1, 2, 3, 4)
> y = c(2, 3, 5, 4)
> r = glm(y ~ x, family=poisson(link="identity"))
> summary(r)

Call:
glm(formula = y ~ x, family = poisson(link = "identity"))

Deviance Residuals:
       1         2         3         4
-0.11496  -0.03194   0.51015  -0.39066

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   1.2784     1.9766   0.647    0.518
x             0.8887     0.8141   1.092    0.275

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1.4716  on 3  degrees of freedom
Residual deviance: 0.4271  on 2  degrees of freedom
AIC: 16.779

Number of Fisher Scoring iterations: 5
```

```
> lambda = predict(r)
> 2 * sum((y*log(y)-y) - (y*log(lambda)-lambda))
[1] 0.4270962
```

```
> 2 * sum((y*log(y)-y) - (y*log(3.5)-3.5))
[1] 1.471633
```

```
ci = sapply(1:4, function(i){poisson.test(y[i])$conf.int})
r = glm(y ~ x, family=poisson(link="identity"))
plot(x, y, type="p", pch=16, xlim=c(0,5), ylim=range(c(0,ci)))
abline(r)
arrows(x, ci[1,], x, ci[2,], length=0.05, angle=90, code=3)
```

```
2 * sum((y*log(y)-y) - (y*log(lambda)-lambda), na.rm=TRUE)
```

```
p = c(0.01, 1, 1, 0.1)
```

```
x = c(1, 2, 3, 4)
```

```
y = c(0, 12, 9, 1)
```

```
z = y / p
```

```
r1 = lm(z ~ x)
plot(x, z, type="p", pch=16, xlim=c(0.5,4.5), ylim=c(0,15))
abline(r1)
```

```
r2 = lm(z ~ x, weights=p)
abline(r2, lty=2)
```

```
q = x * p
r3 = glm(y ~ q + p - 1, family=poisson(link="identity"))
abline(r3$coef['p'], r3$coef['q'], lty=3)
```

### 9.7 로지스틱 회귀

```
set.seed(1)
x = matrix(round(rnorm(1000,mean=50,sd=10)), ncol=10)
invlogit = function(x){exp(x)/(1+exp(x))}
y = sapply(1:100, function(i){rbinom(1, 1, invlogit((x[i,]-50) %*% (1:10)/100))})
```

```
r1 = glm(y ~ x[,1]+x[,2]+x[,3]+x[,4]+x[,5]+x[,6]+x[,7]+x[,8]+x[,9]+x[,10],
         family=binomial(link="logit"))
```

```
> summary(r1)

Call:
glm(formula = y ~ x[, 1] + x[, 2] + x[, 3] + x[, 4] + x[, 5] + x[, 6] +
    x[, 7] + x[, 8] + x[, 9] + x[, 10], family = binomial(link = "logit"))

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-2.63394  -0.50453  -0.05924   0.44280   2.00046

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -37.75406    8.42380  -4.482  7.4e-06 ***
x[, 1]        0.03587    0.03833   0.936 0.349244
x[, 2]       -0.01386    0.03479  -0.398 0.690441
x[, 3]        0.04029    0.02963   1.360 0.173892
x[, 4]        0.08904    0.03645   2.443 0.014576 *
x[, 5]        0.09239    0.03604   2.564 0.010356 *
x[, 6]        0.11091    0.03796   2.922 0.003480 **
x[, 7]        0.05678    0.03159   1.798 0.072239 .
x[, 8]        0.09457    0.03746   2.524 0.011591 *
x[, 9]        0.15957    0.04533   3.520 0.000432 ***
x[, 10]       0.08699    0.03274   2.657 0.007889 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.269  on 99  degrees of freedom
Residual deviance:  69.291  on 89  degrees of freedom
AIC: 91.291

Number of Fisher Scoring iterations: 6
```

```
> install.packages("bestglm")
> library(bestglm)
> Xy = data.frame(x, y)
> bestglm(Xy, family=binomial(link="logit"), IC="AIC")
```

```
> step(r1)
Start:  AIC=91.29
y ~ x[, 1] + x[, 2] + x[, 3] + x[, 4] + x[, 5] + x[, 6] + x[,
    7] + x[, 8] + x[, 9] + x[, 10]

...

Step:  AIC=87.92
y ~ x[, 4] + x[, 5] + x[, 6] + x[, 7] + x[, 8] + x[, 9] + x[,
    10]

...
```

```
r1 = lm(y ~ x[,1]+x[,2]+x[,3]+x[,4]+x[,5]+x[,6]+x[,7]+x[,8]+x[,9]
+x[,10])
r2 = step(r1)
...
```

```
r = glm(y ~ x[,4] + x[,5] + x[,6] + x[,7] + x[,8] + x[,9] + x[,10], family=binomial(link="logit"))
summary(r)
```

```
Call:
glm(formula = y ~ x[, 4] + x[, 5] + x[, 6] + x[, 7] + x[, 8] +
    x[, 9] + x[, 10], family = binomial(link = "logit"))

Deviance Residuals:
     Min        1Q    Median        3Q       Max
-2.17312  -0.53009  -0.09058   0.46361   1.96882

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -32.42913    6.80960  -4.762 1.91e-06 ***
x[, 4]        0.08389    0.03498   2.398 0.016464 *
x[, 5]        0.08887    0.03315   2.681 0.007347 **
x[, 6]        0.10510    0.03747   2.805 0.005035 **
x[, 7]        0.05809    0.02937   1.978 0.047950 *
x[, 8]        0.08352    0.03587   2.328 0.019893 *
x[, 9]        0.15230    0.04363   3.491 0.000482 ***
x[, 10]       0.07626    0.02950   2.585 0.009733 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.269  on 99  degrees of freedom
Residual deviance:  71.922  on 92  degrees of freedom
AIC: 87.922

Number of Fisher Scoring iterations: 6
```

```
plot(fitted(r), y, ylim=c(-0.2, 1.2), yaxt="n")
axis(2, c(0, 1))
```

### 9.8 ROC 곡선

```
> table(fitted(r) >= 0.5, y)
       y
         0  1
  FALSE 43  8
  TRUE  10 39
```

```
> ROC(fitted(r), y)
AUC = 0.9189081 th = 0.2447937
BER = 0.1533521 OR = 62.67857
         Actual
Predicted  0  1
    FALSE 39  2
    TRUE  14 45
```
